---
title: 推广搜4：传统推荐模型之FM模型
date: 2025-01-02 18:24:51
tags:
categories:
  - 深度学习算法
mathjax: true
---

> 本文假设有n个特征

# 一、POLY2模型

前面学到的**逻辑回归模型**只对多维特征进行**简单线性加权，没有多维特征的组合**（逻辑回归模型只能通过工程师手动组合特征）。

而**POLY2模型、FM模型、FFM模型具备特征交叉的能力，其表达能力更强**。

<img src="CombineChareters.png" width="80%" height="80%">

传统的**逻辑回归**模型的预测函数为：

$$y=w_0+\sum_{i}^n w_i x_i$$

**POLY2模型**会为每个特征组合赋予一个权重，因此其n个特征需要$n^2$个参数。其复杂度为$O(n^2)$，预测结果在线性模型的基础上多了一个多项式：

$$y=\sum_{i=1}^{n-1}\sum_{j=i+1}^n w_{i,j} x_i x_j$$

**缺点：**

这种属于暴力组合，由于互联网数据本身通过**one-hot**成的**稀疏数据**，因此通过暴力组合增加维度后会更加稀疏，导致模型**计算量剧增、泛化能力差**。

**解决办法：**可以通过**矩阵分解出隐向量**的方式来降低计算量、提高模型泛化能力。**FM模型**就是通过这种方式构建的。

# 二、FM模型

关于**特征分解隐向量**的相关基础知识参考博客[推广搜2：传统推荐模型之协同过滤算法](https://akirazheng.github.io/2024/12/24/%E6%8E%A8%E5%B9%BF%E6%90%9C2%EF%BC%9A%E4%BC%A0%E7%BB%9F%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%E4%B9%8B%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/)

POLY2为每个特征组合赋予一个权重，相当于每个特征都有一个对应其他所有特征的向量组合，也就是向量矩阵大小为$n\times n$。

FM模型则通过**矩阵分解**的方式将特征向量分解为隐向量，其隐向量大小为$n\times k$，每两个特征向量的权值$w_ij$是由这两个向量的隐向量内积计算而得的：

$$w_{ij}=\mathbf{v_i}\mathbf{v_j^T}$$

因此带入到前面的POLY2模型中，FM的预测函数为：

$$y=\sum_{i=1}^{n-1}\sum_{j=i+1}^n \mathbf{v_i}\mathbf{v_j^T} x_jx_j$$

# 三、FFM模型

FM模型中，**每个特征向量$v_i$都只有一个隐向量**，也就是说，当某个特征$i$跟其他任意特征$j$交互时，发挥的都是相同的作用（因为使用的是同一个隐向量$\vec{v_i}$。

但是很多时候一个特征对不同特征发挥的作用效果是不同的，为了解决这个问题，FFM提出了**域field**的概念，也就是将每个特征从一个隐向量扩展为每个特征有**一组隐向量**，缺点是这也增加了计算复杂度，从**FM模型**到**FFM模型**复杂度由$O(nk)$变为$O(n^2k)$。

**FFM模型**的预测函数为：

$$y=\sum_{i=1}^{n-1}\sum_{j=i+1}^n\mathbf{v_{i,f_j}}\mathbf{v_{j,f_i}^T} x_jx_j$$

其中$\mathbf{v_{i,f_j}}$表示在总共存在的**n个隐向量矩阵**中，特征$i$作用于特征$j$上对应的隐向量为第$j$张矩阵的第$i$行。

<img src="POLY_FM_FFM.png" width="80%" height="80%">


**缺点：**FM和FFM一般只能做到二阶特征交叉（也就是特征两两交叉），由FM的二阶变为三阶，其模型参数计算量剧增。

**解决办法：**通过如**GBDT+LR**的**组合模型**来解决高阶特征交叉的问题。

> 参考
> 
> [推荐算法(一)——FM因式分解(原理+代码)](https://zhuanlan.zhihu.com/p/342803984)
> 
> [推荐算法(二)——FFM原理浅析及代码实战](https://zhuanlan.zhihu.com/p/348596108)
> 
> [包括多种基础推搜算法的github代码仓库](https://github.com/jc-LeeHub/Recommend-System-tf2.0)